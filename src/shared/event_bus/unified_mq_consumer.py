"""
çµ±ä¸€çš„ MQ æ¶ˆè²»è€… (Unified MQ Consumer)

ã€æœ€å°å¯è¡ŒåŸå‰‡ MVPã€‘
- é€™æ˜¯ä»€éº¼ï¼šå¾è¨Šæ¯ä½‡åˆ—æ¥æ”¶äº‹ä»¶ä¸¦è·¯ç”±åˆ°å°æ‡‰è™•ç†å™¨çš„çµ±ä¸€æ¥å£
- ç‚ºä»€éº¼éœ€è¦ï¼šé¿å…é‡è¤‡çš„æ¶ˆè²»è€…ä»£ç¢¼ï¼Œçµ±ä¸€äº‹ä»¶è™•ç†é‚è¼¯
- æ ¸å¿ƒæ¦‚å¿µï¼šæ¶ˆè²»è€…æ¨¡å¼ + äº‹ä»¶è·¯ç”± + è™•ç†å™¨è¨»å†Š
- ä½¿ç”¨å ´æ™¯ï¼šbookingæœå‹™æ¥æ”¶ticketingæœå‹™çš„å›æ‡‰äº‹ä»¶
"""

import asyncio
import base64
import queue
import threading
from typing import Any, Dict, List, Optional

from google.protobuf.json_format import MessageToDict
import orjson
from quixstreams import Application

from src.shared.config.core_setting import settings
from src.shared.logging.loguru_io import Logger


class UnifiedEventConsumer:
    """
    çµ±ä¸€çš„äº‹ä»¶æ¶ˆè²»è€… (Quix Streams ç‰ˆæœ¬)

    ã€Quix Streams å„ªå‹¢ã€‘
    1. ç°¡åŒ–çš„ APIï¼Œæ¸›å°‘æ¨£æ¿ä»£ç¢¼
    2. å…§å»ºç•°æ­¥æ”¯æŒå’ŒéŒ¯èª¤è™•ç†
    3. è‡ªå‹• offset ç®¡ç†å’Œé‡è©¦æ©Ÿåˆ¶
    4. æ›´å¥½çš„æ€§èƒ½å’Œè³‡æºç®¡ç†
    """

    @Logger.io
    def __init__(
        self,
        topics: List[str],
        consumer_group_id: str = 'ticketing-system',
        consumer_tag: str = '[CONSUMER]',
        assigned_partitions: Optional[List[int]] = None,  # æ–°å¢ï¼šæŒ‡å®špartition
    ):
        """
        åˆå§‹åŒ–çµ±ä¸€äº‹ä»¶æ¶ˆè²»è€…

        Args:
            topics: è¦è¨‚é–±çš„Kafkaä¸»é¡Œåˆ—è¡¨
            consumer_group_id: Kafkaæ¶ˆè²»è€…çµ„ID
            consumer_tag: æ¶ˆè²»è€…æ¨™è­˜ï¼Œç”¨æ–¼æ—¥èªŒè¿½è¹¤
        """

        self.topics = topics
        self.consumer_group_id = consumer_group_id
        self.consumer_tag = consumer_tag
        self.assigned_partitions = assigned_partitions  # å„²å­˜æŒ‡å®šçš„partition
        self.running = False
        self.handlers: List[Any] = []

        # æ–°æ¶æ§‹ï¼šæ¶ˆæ¯éšŠåˆ—å’Œç•°æ­¥è™•ç†å™¨
        self.message_queue = queue.Queue()
        self.worker_task = None

        # åˆå§‹åŒ– Quix Applicationï¼ˆä½¿ç”¨æ–°çš„ Consumer Group ID ä»¥é‡æ–°è™•ç†æ¶ˆæ¯ï¼‰
        import uuid

        new_consumer_group = f'{consumer_group_id}-{uuid.uuid4().hex[:8]}'
        Logger.base.info(
            f'\033[93mğŸ”„ [CONSUMER] ä½¿ç”¨æ–°çš„ Consumer Group: {new_consumer_group}\033[0m'
        )

        self.app = Application(
            broker_address=settings.KAFKA_BOOTSTRAP_SERVERS,
            consumer_group=new_consumer_group,
            auto_offset_reset='latest',  # å¾æœ€æ–°æ¶ˆæ¯é–‹å§‹ï¼Œè·³éæœ‰å•é¡Œçš„èˆŠæ¶ˆæ¯
            processing_guarantee='exactly-once',  # å•Ÿç”¨ exactly-once èªç¾©
            consumer_extra_config={
                'enable.auto.commit': True,
                'auto.commit.interval.ms': 1000,
                'session.timeout.ms': 30000,
                'heartbeat.interval.ms': 10000,
            },
        )

    @Logger.io
    def register_handler(self, handler: Any) -> None:
        """è¨»å†Šäº‹ä»¶è™•ç†å™¨"""
        self.handlers.append(handler)

    @Logger.io
    async def start(self):
        """
        å•Ÿå‹• Quix Streams æ¶ˆè²»è€… (ç´” Protobuf ç‰ˆæœ¬)

        ã€Protobuf æµç¨‹ã€‘
        1. å‰µå»º Protobuf topics å’Œ streaming dataframe
        2. è¨­ç½®è‡ªå‹•ååºåˆ—åŒ–æµæ°´ç·š
        3. é‹è¡Œ streaming application
        """
        try:
            from quixstreams.models.serializers.protobuf import ProtobufDeserializer

            import src.shared.event_bus.proto.domain_event_pb2 as domain_event_pb2

            # Protobuf class with type stub support
            DomainEventProtoBufClass = domain_event_pb2.DomainEvent

            # ç‚ºæ¯å€‹ topic å‰µå»º Protobuf topic å°è±¡
            quix_topics = []
            for topic_name in self.topics:
                topic = self.app.topic(
                    name=topic_name,
                    value_deserializer=ProtobufDeserializer(
                        msg_type=DomainEventProtoBufClass,
                        to_dict=False,  # ä¿æŒ Protobuf å°è±¡æ ¼å¼ # è«‹å‹¿æ›´å‹•
                    ),
                    key_deserializer='str',
                )
                quix_topics.append(topic)

            # ç‚ºæ¯å€‹ topic å‰µå»º streaming dataframe
            Logger.base.info(f'ğŸ”— [CONSUMER] è¨­ç½® topic ç›£è½: {[t.name for t in quix_topics]}')

            for i, topic in enumerate(quix_topics):
                Logger.base.info(f'ğŸ”— [CONSUMER] è™•ç† topic {i}: {topic.name}')
                topic_sdf = self.app.dataframe(topic=topic)
                topic_sdf = topic_sdf.apply(self._deserialize_message)  # è½‰æ›ç‚ºå­—å…¸
                topic_sdf = topic_sdf.apply(self._log_before_filter)  # è¨˜éŒ„éæ¿¾å‰çš„æ•¸æ“š
                topic_sdf = topic_sdf.filter(
                    lambda x: x.get('event_type') is not None
                )  # éæ¿¾æœ‰æ•ˆäº‹ä»¶
                topic_sdf = topic_sdf.apply(self._collect_message_sync)

            Logger.base.info(f'âœ… [CONSUMER] ç›£è½æ‰€æœ‰ topics: {self.topics}')

            self.running = True
            Logger.base.info(
                f'{self.consumer_tag} Quix Streams Consumer started for topics: {self.topics}'
            )

            # åœ¨å¾Œå°ç·šç¨‹å•Ÿå‹•ç•°æ­¥æ¶ˆæ¯è™•ç†å™¨

            def start_worker():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(self._async_message_worker())

            worker_thread = threading.Thread(target=start_worker, daemon=True)
            worker_thread.start()
            Logger.base.info('ğŸš€ [WORKER] ç•°æ­¥è™•ç†å™¨å·²åœ¨å¾Œå°ç·šç¨‹å•Ÿå‹•')

            self.app.run()

        except Exception as e:
            Logger.base.error(f'{self.consumer_tag} Failed to start Quix Streams Consumer: {e}')
            raise

    @Logger.io
    async def stop(self):
        """åœæ­¢ Quix Streams æ¶ˆè²»è€…"""
        self.running = False

        # åœæ­¢ç•°æ­¥è™•ç†å™¨
        if self.worker_task:
            self.worker_task.cancel()
            try:
                await self.worker_task
            except asyncio.CancelledError:
                pass

        # Quix Streams æœƒè‡ªå‹•è™•ç†è³‡æºæ¸…ç†
        Logger.base.info(f'{self.consumer_tag} Quix Streams Consumer stopped')

    def _deserialize_message(self, message) -> Dict[str, Any]:
        """
        ååºåˆ—åŒ– Protobuf æ¶ˆæ¯ (Quix Streams ç‰ˆæœ¬) - æ”¯æ´æ··åˆæ ¼å¼

        ã€å¥å£¯çš„ååºåˆ—åŒ–ç­–ç•¥ã€‘
        1. é¦–å…ˆå˜—è©¦ Protobuf å°è±¡è™•ç†
        2. è™•ç† Protobuf ä¸­çš„ data å­—æ®µ (ä½¿ç”¨ orjson ååºåˆ—åŒ–)
        3. æ‰€æœ‰å¤±æ•—å‰‡è·³éè©²æ¶ˆæ¯é¿å…é˜»å¡
        """

        try:
            Logger.base.info(f'ğŸ” [CONSUMER] æ”¶åˆ°æ¶ˆæ¯é€²è¡Œååºåˆ—åŒ–: {type(message)}')

            # Step 1: å–å‡º value
            raw_value = message.value if hasattr(message, 'value') else message

            # Step 2: å˜—è©¦ Protobuf
            try:
                event_data = MessageToDict(
                    raw_value,
                    preserving_proto_field_name=True,  # ä¿ç•™åŸå§‹å­—æ®µå
                )
                Logger.base.info(f'ğŸ‰ [CONSUMER] Protobuf ååºåˆ—åŒ–å®Œæˆ: {event_data}')

                # Step 3: è™•ç† Protobuf ä¸­çš„ data å­—æ®µ (ä½¿ç”¨ orjson ååºåˆ—åŒ–)
                if 'data' in event_data and event_data['data']:
                    data_bytes = base64.b64decode(
                        event_data['data']
                    )  # data å­—æ®µæ˜¯ base64 ç·¨ç¢¼çš„ bytesï¼Œéœ€è¦å…ˆè§£ç¢¼å†ç”¨ orjson è§£æ
                    parsed_data = orjson.loads(data_bytes)
                    event_data['data'] = parsed_data
                    Logger.base.info(f'âœ… [CONSUMER] orjson ååºåˆ—åŒ– data å­—æ®µæˆåŠŸ: {parsed_data}')

                return event_data
            except Exception as e:
                Logger.base.critical(f'âš ï¸ Protobuf ååºåˆ—åŒ–å¤±æ•—: {e}')
                Logger.base.critical(f'Unrecognized message format: {type(raw_value)}, skipping...')
                return {'event_type': None, 'error': 'Unrecognized message format'}

        except Exception as e:
            Logger.base.error(f'Critical deserialization error: {e}')
            return {'event_type': None, 'error': str(e)}

    def _log_before_filter(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        Logger.base.info(f'ğŸ” [CONSUMER] éæ¿¾å‰æª¢æŸ¥: {event_data}')
        return event_data

    def _collect_message_sync(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥æ”¶é›†æ¶ˆæ¯åˆ°éšŠåˆ— - Quix Streams å°ˆç”¨"""
        try:
            # åªåšç°¡å–®çš„æ¶ˆæ¯æ”¶é›†ï¼Œä¸è™•ç†æ¥­å‹™é‚è¼¯
            self.message_queue.put(event_data)
            Logger.base.info(f'ğŸ“¨ [CONSUMER] æ¶ˆæ¯å·²æ”¶é›†åˆ°éšŠåˆ—: {event_data.get("event_type")}')
            return {'status': 'collected', 'event_type': event_data.get('event_type')}
        except Exception as e:
            Logger.base.error(f'âŒ [CONSUMER] æ¶ˆæ¯æ”¶é›†å¤±æ•—: {e}')
            return {'status': 'collection_failed', 'error': str(e)}

    @Logger.io
    async def _async_message_worker(self):
        """ç•°æ­¥æ¶ˆæ¯è™•ç†å™¨ - ç¨ç«‹æ–¼ Quix Streams"""
        Logger.base.info('ğŸš€ [WORKER] ç•°æ­¥æ¶ˆæ¯è™•ç†å™¨å•Ÿå‹•')

        while self.running:
            try:
                # éé˜»å¡æ–¹å¼å¾éšŠåˆ—å–æ¶ˆæ¯
                if not self.message_queue.empty():
                    event_data = self.message_queue.get_nowait()
                    await self._process_message_async(event_data)
                else:
                    # æ²’æœ‰æ¶ˆæ¯æ™‚çŸ­æš«ç­‰å¾…
                    await asyncio.sleep(0.1)
            except queue.Empty:
                await asyncio.sleep(0.1)
            except Exception as e:
                Logger.base.error(f'âŒ [WORKER] ç•°æ­¥è™•ç†å™¨éŒ¯èª¤: {e}')
                await asyncio.sleep(1)

    @Logger.io
    async def _process_message_async(self, event_data: Dict[str, Any]):
        """çœŸæ­£çš„ç•°æ­¥æ¶ˆæ¯è™•ç† - æ¥­å‹™é‚è¼¯åœ¨é€™è£¡"""
        event_type = event_data.get('event_type')
        Logger.base.info(f'ğŸš€ [WORKER] è™•ç†äº‹ä»¶: {event_type}')

        # èª¿ç”¨åŸæœ¬çš„ç•°æ­¥ handlers
        for handler in self.handlers:
            try:
                Logger.base.info(f'ğŸ” [WORKER] å˜—è©¦è™•ç†å™¨: {type(handler).__name__}')

                # ç›´æ¥ await async handler
                result = await handler.handle(event_data)

                if result:
                    Logger.base.info(f'âœ… [WORKER] è™•ç†æˆåŠŸ: {type(handler).__name__}')
                    return

            except Exception as e:
                Logger.base.info(f'âš ï¸ [WORKER] è™•ç†å™¨ {type(handler).__name__} è·³é: {e}')
                continue

        Logger.base.warning(f'âš ï¸ [WORKER] æ²’æœ‰è™•ç†å™¨èƒ½è™•ç†äº‹ä»¶: {event_type}')

    async def _process_event_with_handlers(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è™•ç†äº‹ä»¶èˆ‡è™•ç†å™¨è·¯ç”± (ç´”ç•°æ­¥ç‰ˆæœ¬)

        ã€æœ€å°å¯è¡ŒåŸå‰‡ã€‘
        1. åˆ¤æ–·äº‹ä»¶é¡å‹
        2. æ‰¾åˆ°å°æ‡‰çš„ handler
        3. ç›´æ¥åˆ†ç™¼è™•ç†
        4. è¿”å›çµæœ
        """
        event_type = event_data.get('event_type')
        Logger.base.info(f'ğŸš€ [CONSUMER] é–‹å§‹è™•ç†äº‹ä»¶: {event_type}')
        Logger.base.info(f'ğŸ” [CONSUMER] å·²è¨»å†Šçš„è™•ç†å™¨æ•¸é‡: {len(self.handlers)}')

        try:
            # 1. å¿«é€Ÿè·¯ç”±åˆ°æ­£ç¢ºçš„ handler
            target_handler = None
            for i, handler in enumerate(self.handlers):
                handler_name = type(handler).__name__
                Logger.base.info(f'ğŸ” [CONSUMER] æª¢æŸ¥è™•ç†å™¨ {i}: {handler_name}')
                try:
                    # ç›´æ¥èª¿ç”¨ async æ–¹æ³•
                    can_handle = await handler.can_handle(event_type or '')
                    Logger.base.info(
                        f'ğŸ” [CONSUMER] {handler_name}.can_handle("{event_type}") = {can_handle}'
                    )
                    if can_handle:
                        target_handler = handler
                        Logger.base.info(f'âœ… [CONSUMER] æ‰¾åˆ°è™•ç†å™¨: {handler_name}')
                        break
                except Exception as e:
                    Logger.base.error(f'âŒ [CONSUMER] æª¢æŸ¥è™•ç†å™¨ {handler_name} æ™‚å‡ºéŒ¯: {e}')
                    continue

            # 2. å¦‚æœæ²’æ‰¾åˆ° handlerï¼Œç›´æ¥è¿”å›
            if not target_handler:
                Logger.base.warning(f'âš ï¸ [CONSUMER] æ²’æœ‰æ‰¾åˆ°è™•ç†å™¨å°æ‡‰äº‹ä»¶é¡å‹: {event_type}')
                return {'status': 'no_handler', 'event_type': event_type}

            # 3. ç›´æ¥åˆ†ç™¼åˆ° handler è™•ç†
            Logger.base.info(f'ğŸ¯ [CONSUMER] åˆ†ç™¼åˆ° handler: {type(target_handler).__name__}')

            # ç›´æ¥èª¿ç”¨ async æ–¹æ³•
            result = await target_handler.handle(event_data)

            Logger.base.info(f'âœ… [CONSUMER] è™•ç†å®Œæˆ: {result}')
            return {'status': 'processed', 'result': result, 'event_type': event_type}

        except Exception as e:
            Logger.base.error(f'ğŸ’¥ [CONSUMER] è™•ç†äº‹ä»¶å¤±æ•—: {e}')
            return {'status': 'error', 'error': str(e), 'event_type': event_type}


# === å…¨å±€æ¶ˆè²»è€…ç®¡ç† ===
# ã€MVPåŸå‰‡ã€‘ä½¿ç”¨å…¨å±€å¯¦ä¾‹ç®¡ç†ï¼Œé¿å…é‡è¤‡å‰µå»ºæ¶ˆè²»è€…

_unified_consumer: Optional[UnifiedEventConsumer] = None


@Logger.io
async def start_unified_consumer(
    topics: List[str], handlers: List[Any], consumer_tag: str = '[CONSUMER]'
) -> None:
    """
    å•Ÿå‹•çµ±ä¸€çš„äº‹ä»¶æ¶ˆè²»è€…

    ã€MVPå•Ÿå‹•æ¥å£ã€‘é€™æ˜¯å¤–éƒ¨å•Ÿå‹•æ¶ˆè²»è€…çš„çµ±ä¸€å…¥å£

    Args:
        topics: è¦ç›£è½çš„Kafkaä¸»é¡Œåˆ—è¡¨
        handlers: äº‹ä»¶è™•ç†å™¨åˆ—è¡¨ï¼ˆæ¯å€‹æœå‹™æä¾›è‡ªå·±çš„è™•ç†å™¨ï¼‰
        consumer_tag: æ¶ˆè²»è€…æ¨™è­˜ï¼Œç”¨æ–¼æ—¥å¿—è¿½è¹¤

    ä½¿ç”¨ä¾‹å­ï¼š
    ```python
    topics = ["ticketing-booking-request", "ticketing-booking-response"]
    handlers = [BookingEventConsumer(), TicketingEventConsumer()]
    await start_unified_consumer(topics, handlers)
    ```
    """
    global _unified_consumer

    if _unified_consumer is None:
        # å‰µå»ºçµ±ä¸€æ¶ˆè²»è€…å¯¦ä¾‹
        _unified_consumer = UnifiedEventConsumer(topics, consumer_tag=consumer_tag)

        # è¨»å†Šæ‰€æœ‰äº‹ä»¶è™•ç†å™¨
        for handler in handlers:
            _unified_consumer.register_handler(handler)

    # å•Ÿå‹•æ¶ˆè²»è€…
    await _unified_consumer.start()


@Logger.io
async def stop_unified_consumer() -> None:
    """
    åœæ­¢çµ±ä¸€çš„äº‹ä»¶æ¶ˆè²»è€…

    ã€MVPåœæ­¢æ¥å£ã€‘æ¸…ç†å…¨å±€æ¶ˆè²»è€…è³‡æº
    """
    global _unified_consumer
    if _unified_consumer:
        await _unified_consumer.stop()
        _unified_consumer = None


@Logger.io
def get_unified_consumer() -> Optional[UnifiedEventConsumer]:
    """
    ç²å–çµ±ä¸€çš„æ¶ˆè²»è€…å¯¦ä¾‹

    ã€MVPæŸ¥è©¢æ¥å£ã€‘ç”¨æ–¼ç›£æ§å’Œèª¿è©¦
    """
    global _unified_consumer
    return _unified_consumer
