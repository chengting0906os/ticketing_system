"""
Ticketing MQ Consumer - Unified PostgreSQL State Manager
ç¥¨å‹™ MQ æ¶ˆè²»è€… - çµ±ä¸€ PostgreSQL ç‹€æ…‹ç®¡ç†å™¨

æ•´åˆè·è²¬ (2 å€‹ topics)ï¼š
1. Booking + Ticket ç‹€æ…‹åŒæ­¥ (åŸå­æ“ä½œ):
   - pending_payment_and_reserved: åº§ä½é è¨‚æˆåŠŸå¾Œï¼ŒåŒæ™‚æ›´æ–° Booking ç‚º PENDING_PAYMENT å’Œ Ticket ç‚º RESERVED

2. Booking å¤±æ•—è™•ç†:
   - failed: åº§ä½é è¨‚å¤±æ•—å¾Œæ›´æ–°è¨‚å–®ç‹€æ…‹

é‡è¦ï¼š
- é€™å€‹ consumer **åªæ“ä½œ PostgreSQL**ï¼Œä¸ç¢° Kvrocksï¼
- Kvrocks ç‹€æ…‹ç®¡ç†æ˜¯ seat_reservation_consumer çš„è·è²¬
- åˆä½µ topic ç¢ºä¿ Booking å’Œ Ticket ç‹€æ…‹æ›´æ–°çš„åŸå­æ€§

Features:
- éŒ¯èª¤è™•ç†ï¼šä½¿ç”¨ Quix Streams callback è™•ç†éŒ¯èª¤
- æ­»ä¿¡éšŠåˆ—ï¼šç„¡æ³•è™•ç†çš„è¨Šæ¯ç™¼é€è‡³ DLQ
"""

import json
import os
import time
from typing import TYPE_CHECKING, Any, Dict, Optional

from anyio.from_thread import BlockingPortal, start_blocking_portal
from opentelemetry import trace
from opentelemetry.propagate import extract
from opentelemetry.trace import SpanKind
from quixstreams import Application


if TYPE_CHECKING:
    from anyio.from_thread import BlockingPortal

from src.platform.config.core_setting import settings
from src.platform.logging.loguru_io import Logger
from src.platform.message_queue.kafka_constant_builder import (
    KafkaConsumerGroupBuilder,
    KafkaTopicBuilder,
)
from src.service.ticketing.app.command.update_booking_status_to_failed_use_case import (
    UpdateBookingToFailedUseCase,
)
from src.service.ticketing.app.command.update_booking_status_to_pending_payment_and_ticket_to_reserved_use_case import (
    UpdateBookingToPendingPaymentAndTicketToReservedUseCase,
)
from src.service.ticketing.driven_adapter.repo.booking_command_repo_scylla_impl import (
    BookingCommandRepoScyllaImpl,
)
from src.service.ticketing.driven_adapter.repo.booking_query_repo_scylla_impl import (
    BookingQueryRepoScyllaImpl,
)


class KafkaConfig:
    """Kafka é…ç½® - æ”¯æ´ Exactly-Once èªç¾©"""

    def __init__(self, *, event_id: int, instance_id: str, retries: int = 3):
        """
        Args:
            event_id: æ´»å‹• ID
            instance_id: Consumer instance ID (ç”¨æ–¼ç”Ÿæˆå”¯ä¸€çš„ transactional.id)
            retries: Producer é‡è©¦æ¬¡æ•¸
        """
        from src.platform.message_queue.kafka_constant_builder import (
            KafkaProducerTransactionalIdBuilder,
        )

        self.event_id = event_id
        self.instance_id = instance_id
        self.retries = retries
        self.transactional_id = KafkaProducerTransactionalIdBuilder.ticketing_service(
            event_id=event_id, instance_id=instance_id
        )

    @property
    def producer_config(self) -> Dict:
        """
        Producer é…ç½® - å•Ÿç”¨äº‹å‹™æ”¯æ´

        Note: Quix Streams with processing_guarantee='exactly-once' requires:
        - transactional.id: å”¯ä¸€è­˜åˆ¥æ­¤ producerï¼Œå¯¦ç¾ exactly-once
        - enable.idempotence = True (è‡ªå‹•è¨­ç½®)
        - acks = 'all' (è‡ªå‹•è¨­ç½®)
        """
        return {
            'transactional.id': self.transactional_id,  # ğŸ”‘ Exactly-Once çš„é—œéµ
            'retries': self.retries,
        }

    @property
    def consumer_config(self) -> Dict:
        """
        Consumer é…ç½®

        Note: Quix Streams with processing_guarantee='exactly-once' already sets:
        - enable.auto.commit = False (manual commit via transactions)
        - isolation.level = 'read_committed' (only read committed messages)

        We only set auto.offset.reset for first-time startup behavior:
        - 'latest': Skip old messages, start from newest (recommended for production)
        - 'earliest': Process all messages from beginning (use for testing/recovery)
        """
        return {
            'auto.offset.reset': 'latest',
        }


class TicketingMqConsumer:
    """
    æ•´åˆçš„ç¥¨å‹™ MQ æ¶ˆè²»è€… (PostgreSQL ç‹€æ…‹ç®¡ç†)

    è™•ç† 2 å€‹ topicsï¼š
    - Booking + Ticket åŸå­æ›´æ–° (pending_payment + reserved)
    - Booking å¤±æ•—è™•ç† (failed)

    å…¨éƒ¨éƒ½æ˜¯ PostgreSQL æ“ä½œï¼Œç„¡ç‹€æ…‹è™•ç†
    """

    def __init__(self):
        self.event_id = int(os.getenv('EVENT_ID', '1'))
        # Generate unique instance_id per worker process using PID to avoid transactional.id conflicts
        base_instance_id = settings.KAFKA_CONSUMER_INSTANCE_ID
        self.instance_id = f'{base_instance_id}-pid-{os.getpid()}'
        self.consumer_group_id = os.getenv(
            'CONSUMER_GROUP_ID',
            KafkaConsumerGroupBuilder.ticketing_service(event_id=self.event_id),
        )

        self.kafka_config = KafkaConfig(event_id=self.event_id, instance_id=self.instance_id)
        self.kafka_app: Optional[Application] = None
        self.running = False
        self.portal: Optional['BlockingPortal'] = None
        self.tracer = trace.get_tracer(__name__)

        # DLQ configuration
        self.dlq_topic = KafkaTopicBuilder.ticketing_dlq(event_id=self.event_id)

        # Use cases (å»¶é²åˆå§‹åŒ–)
        self.update_booking_to_pending_payment_use_case: Any = None
        self.update_booking_to_failed_use_case: Any = None

    def set_portal(self, portal: 'BlockingPortal') -> None:
        """è¨­ç½® BlockingPortal ç”¨æ–¼åŒæ­¥èª¿ç”¨ async å‡½æ•¸"""
        self.portal = portal

    def _on_processing_error(self, exc: Exception, row: Any, _logger: Any) -> bool:
        """
        Quix Streams éŒ¯èª¤è™•ç† callback

        ç•¶è¨Šæ¯è™•ç†å¤±æ•—æ™‚ï¼Œæ­¤ callback æœƒè¢«èª¿ç”¨ã€‚
        éŒ¯èª¤è¨Šæ¯ç›´æ¥ç™¼é€åˆ° DLQï¼Œä¸åœ¨æ­¤å±¤é€²è¡Œé‡è©¦ã€‚

        Returns:
            True: å¿½ç•¥éŒ¯èª¤ï¼Œæäº¤ offsetï¼ˆè¨Šæ¯è¢«ç™¼é€åˆ° DLQï¼‰
            False: å‚³æ’­éŒ¯èª¤ï¼Œä¸æäº¤ offsetï¼ˆåœæ­¢ consumerï¼‰
        """
        error_msg = str(exc)

        Logger.base.error(f'âŒ [TICKETING-ERROR-CALLBACK] Processing error, sending to DLQ: {exc}')

        # ç™¼é€åˆ° DLQ
        if row and hasattr(row, 'value'):
            message = row.value
            self._send_to_dlq(
                message=message,
                original_topic='unknown',  # Quix doesn't provide topic in callback
                error=error_msg,
                retry_count=0,
            )

        # è¿”å› Trueï¼šæäº¤ offsetï¼Œè¨Šæ¯å·²ç™¼é€åˆ° DLQ
        return True

    @Logger.io
    def _create_kafka_app(self) -> Application:
        """å‰µå»ºæ”¯æ´ Exactly-Once çš„ Kafka æ‡‰ç”¨ï¼Œé…ç½®éŒ¯èª¤è™•ç†"""
        app = Application(
            broker_address=settings.KAFKA_BOOTSTRAP_SERVERS,
            consumer_group=self.consumer_group_id,
            processing_guarantee='exactly-once',  # ğŸ†• å•Ÿç”¨ exactly-once è™•ç†
            commit_interval=0,  # ğŸ†• ç¦ç”¨è‡ªå‹•æäº¤é–“éš”ï¼Œè®“äº‹å‹™ç®¡ç†
            producer_extra_config=self.kafka_config.producer_config,
            consumer_extra_config=self.kafka_config.consumer_config,
            on_processing_error=self._on_processing_error,  # ğŸ†• éŒ¯èª¤è™•ç† callback
        )

        Logger.base.info(
            f'ğŸ« [TICKETING] Created exactly-once Kafka app\n'
            f'   ğŸ‘¥ Group: {self.consumer_group_id}\n'
            f'   ğŸ« Event: {self.event_id}\n'
            f'   ğŸ”’ Processing: exactly-once\n'
            f'   ğŸ”‘ Transactional ID: {self.kafka_config.transactional_id}\n'
            f'   âš ï¸ Error handling: enabled'
        )
        return app

    @Logger.io
    def _setup_topics(self):
        """è¨­ç½® 2 å€‹ topic çš„è™•ç†é‚è¼¯ - ä½¿ç”¨ Kafka äº‹å‹™å¯¦ç¾ Exactly Once"""
        if not self.kafka_app:
            self.kafka_app = self._create_kafka_app()

        # å®šç¾© topic é…ç½®
        topics = {
            'pending_payment_and_reserved': (
                KafkaTopicBuilder.update_booking_status_to_pending_payment_and_ticket_status_to_reserved_in_postgresql(
                    event_id=self.event_id
                ),
                self._process_pending_payment_and_reserved,
            ),
            'failed': (
                KafkaTopicBuilder.update_booking_status_to_failed(event_id=self.event_id),
                self._process_failed,
            ),
        }

        # è¨»å†Šæ‰€æœ‰ topics - ä½¿ç”¨ stateless æ¨¡å¼ï¼Œä¾è³´ Kafka äº‹å‹™
        for name, (topic_name, handler) in topics.items():
            topic = self.kafka_app.topic(
                name=topic_name,
                key_serializer='str',
                value_serializer='json',
            )

            # ä½¿ç”¨ stateless è™•ç†ï¼Œä¾è³´ Kafka äº‹å‹™çš„ exactly once ä¿è­‰
            self.kafka_app.dataframe(topic=topic).apply(handler, stateful=False)
            Logger.base.info(f'   âœ“ {name.capitalize()} topic configured (stateless + transaction)')

        Logger.base.info('âœ… All topics configured (exactly once via Kafka transactions)')

    # ========== DLQ Helper ==========

    @Logger.io
    def _send_to_dlq(self, *, message: Dict, original_topic: str, error: str, retry_count: int):
        """ç™¼é€å¤±æ•—è¨Šæ¯åˆ° DLQ"""
        if not self.kafka_app:
            Logger.base.error('âŒ [TICKETING-DLQ] Kafka app not initialized')
            return

        try:
            # æ§‹å»º DLQ è¨Šæ¯ï¼ˆåŒ…å«åŸå§‹è¨Šæ¯å’ŒéŒ¯èª¤ä¿¡æ¯ï¼‰
            dlq_message = {
                'original_message': message,
                'original_topic': original_topic,
                'error': error,
                'retry_count': retry_count,
                'timestamp': time.time(),
                'instance_id': self.instance_id,
            }

            # ç™¼é€åˆ° DLQï¼ˆä½¿ç”¨ booking_id ä½œç‚º keyï¼Œä¿æŒé †åºï¼‰
            serialized_message = json.dumps(dlq_message).encode('utf-8')

            with self.kafka_app.get_producer() as producer:
                producer.produce(
                    topic=self.dlq_topic,
                    key=str(message.get('booking_id', 'unknown')),
                    value=serialized_message,
                )

            Logger.base.warning(
                f'ğŸ“® [TICKETING-DLQ] Sent to DLQ: booking_id={message.get("booking_id")} '
                f'after {retry_count} retries, error: {error}'
            )

        except Exception as e:
            Logger.base.error(f'âŒ [TICKETING-DLQ] Failed to send to DLQ: {e}')

    # ========== Message Handlers ==========

    def _process_pending_payment_and_reserved(
        self, message: Dict, key: Any = None, context: Any = None
    ) -> Dict:
        """è™•ç† Booking â†’ PENDING_PAYMENT + Ticket â†’ RESERVED (åŸå­æ“ä½œ)"""
        booking_id = message.get('booking_id')
        reserved_seats = message.get('reserved_seats', [])

        # Extract partition info from Quix Streams context
        partition_info = ''
        if hasattr(context, 'partition'):
            partition_info = f' | partition={context.partition}'

        # Extract trace context from Kafka message headers
        headers_dict = {}
        if hasattr(context, 'headers') and context.headers:
            # Convert Kafka headers (list of tuples) to dict
            for header_key, header_value in context.headers:
                if isinstance(header_value, bytes):
                    headers_dict[header_key] = header_value.decode('utf-8')
                else:
                    headers_dict[header_key] = header_value

        # Extract parent context from headers
        parent_context = extract(headers_dict)

        # Create a linked span (connects to parent trace from HTTP request)
        with self.tracer.start_as_current_span(
            'CONSUMER.PROCESS_PENDING_PAYMENT_AND_RESERVED',
            context=parent_context,
            kind=SpanKind.CONSUMER,
            attributes={
                'messaging.system': 'kafka',
                'messaging.operation': 'receive',
                'booking_id': booking_id or 0,
                'event_id': message.get('event_id') or 0,
                'buyer_id': message.get('buyer_id') or 0,
                'reserved_seats_count': len(reserved_seats),
                'consumer.instance_id': self.instance_id,
            },
        ):
            try:
                Logger.base.info(
                    f'ğŸ“¥ [BOOKING+TICKET-{self.instance_id}] Processing: booking_id={booking_id}{partition_info}'
                )

                with self.tracer.start_as_current_span('consumer.call_async_handler'):
                    # Use portal to call async function (ensures proper async context)
                    # pyrefly: ignore  # missing-attribute
                    self.portal.call(self._handle_pending_payment_and_reserved_async, message)

                Logger.base.info(
                    f'âœ… [BOOKING+TICKET] Completed: booking_id={booking_id}, tickets={len(reserved_seats)}'
                )
                return {'success': True}

            except Exception as e:
                Logger.base.error(f'âŒ [BOOKING+TICKET] Failed: booking_id={booking_id}, error={e}')
                trace.get_current_span().set_attribute('error', True)
                trace.get_current_span().set_attribute('error.message', str(e))
                return {'success': False, 'error': str(e)}

    async def _handle_pending_payment_and_reserved_async(self, message: Dict[str, Any]):
        """
        Async handler for pending payment and reserved

        Note: Repositories use asyncpg and manage their own connections.
        Use case directly depends on repositories, no UoW needed.
        """
        Logger.base.info(f'ğŸ”§ [BOOKING+TICKET] Handling async message: {message} ')

        with self.tracer.start_as_current_span(
            'consumer.handle_pending_payment_async',
            attributes={
                'booking_id': message.get('booking_id') or 0,
                'event_id': message.get('event_id') or 0,
                'buyer_id': message.get('buyer_id') or 0,
            },
        ):
            booking_id = message.get('booking_id')
            buyer_id = message.get('buyer_id')
            event_id = message.get('event_id')
            reserved_seats = message.get('reserved_seats', [])
            ticket_details = message.get(
                'ticket_details', []
            )  # æ–°å¢ï¼šç²å– ticket è©³ç´°è³‡è¨Šï¼ˆå«åƒ¹æ ¼ï¼‰

            # Extract section and subsection from first seat
            # Seat format: 'section-subsection-row-seat' (e.g., 'A-1-1-3')
            with self.tracer.start_as_current_span('consumer.parse_seat_data'):
                section = None
                subsection = None
                if reserved_seats:
                    first_seat = reserved_seats[0]
                    parts = first_seat.split('-')
                    if len(parts) == 4:
                        section = parts[0]
                        subsection = int(parts[1])

                # Convert seat identifiers from 'section-subsection-row-seat' to 'row-seat' format
                # Seat Reservation Service sends: ['A-1-1-3', 'A-1-1-4']
                # Atomic CTE expects: ['1-3', '1-4']
                seat_identifiers = []
                for seat_id in reserved_seats:
                    parts = seat_id.split('-')
                    if len(parts) == 4:  # section-subsection-row-seat
                        row_seat = f'{parts[2]}-{parts[3]}'  # Extract row-seat only
                        seat_identifiers.append(row_seat)
                    else:
                        Logger.base.warning(
                            f'âš ï¸ [BOOKING+TICKET] Invalid seat format: {seat_id} (expected section-subsection-row-seat)'
                        )

                # Extract ticket price from ticket_details (all tickets have same price per subsection)
                # ticket_details format: [{'seat_id': 'A-1-1-1', 'price': 1000}, ...]
                if not ticket_details or len(ticket_details) == 0:
                    Logger.base.error(
                        f'âŒ [BOOKING+TICKET] No ticket_details in message for booking {booking_id}'
                    )
                    raise ValueError('ticket_details is required in SeatsReservedEvent')

                ticket_price = ticket_details[0].get('price')
                if ticket_price is None:
                    Logger.base.error(
                        f'âŒ [BOOKING+TICKET] No price in ticket_details for booking {booking_id}'
                    )
                    raise ValueError('price is required in ticket_details')

            # Create repository (asyncpg-based, no session management needed)
            with self.tracer.start_as_current_span('consumer.create_repo_and_usecase'):
                booking_command_repo = BookingCommandRepoScyllaImpl()

                # Create and execute use case with direct repository injection
                use_case = UpdateBookingToPendingPaymentAndTicketToReservedUseCase(
                    booking_command_repo=booking_command_repo,
                )

            with self.tracer.start_as_current_span(
                'consumer.execute_usecase',
                attributes={
                    'section': section or '',
                    'subsection': subsection or 0,
                    'seat_count': len(seat_identifiers),
                    'ticket_price': ticket_price,
                },
            ):
                await use_case.execute(
                    booking_id=booking_id or 0,
                    buyer_id=buyer_id or 0,
                    event_id=event_id or 0,
                    section=section or '',
                    subsection=subsection or 0,
                    seat_identifiers=seat_identifiers,
                    ticket_price=ticket_price,
                )

    @Logger.io
    def _process_failed(self, message: Dict, key: Any = None, context: Any = None) -> Dict:
        """è™•ç† Booking â†’ FAILED"""
        booking_id = message.get('booking_id')

        # Extract partition info from Quix Streams context
        partition_info = ''
        if hasattr(context, 'partition'):
            partition_info = f' | partition={context.partition}'

        try:
            Logger.base.info(
                f'ğŸ“¥ [BOOKING-FAILED-{self.instance_id}] Processing: booking_id={booking_id}{partition_info}'
            )

            # Use portal to call async function (ensures proper async context)
            # pyrefly: ignore  # missing-attribute
            self.portal.call(self._handle_failed_async, message)

            Logger.base.info(f'âœ… [BOOKING-FAILED] Completed: {booking_id}')
            return {'success': True}

        except Exception as e:
            Logger.base.error(f'âŒ [BOOKING-FAILED] Failed: booking_id={booking_id}, error={e}')
            return {'success': False, 'error': str(e)}

    async def _handle_failed_async(self, message: Dict[str, Any]):
        """
        Async handler for failed booking

        Note: Repositories use asyncpg and manage their own connections.
        Use case directly depends on repositories, no UoW needed.
        """
        booking_id = message.get('booking_id')
        buyer_id = message.get('buyer_id')
        reason = message.get('error_message', 'Unknown')

        # Create repositories (they manage their own asyncpg connections)
        booking_command_repo = BookingCommandRepoScyllaImpl()
        booking_query_repo = BookingQueryRepoScyllaImpl()

        # Create and execute use case with direct repository injection
        use_case = UpdateBookingToFailedUseCase(
            booking_query_repo=booking_query_repo,
            booking_command_repo=booking_command_repo,
        )
        await use_case.execute(
            booking_id=booking_id or 0, buyer_id=buyer_id or 0, error_message=reason
        )

    # ========== Lifecycle ==========

    def start(self):
        """å•Ÿå‹•æœå‹™ - æ”¯æ´ topic metadata åŒæ­¥é‡è©¦"""
        import time

        max_retries = 5
        retry_delay = 2  # seconds

        for attempt in range(1, max_retries + 1):
            try:
                # è¨­ç½® Kafka topics
                self._setup_topics()

                Logger.base.info(
                    f'ğŸš€ [TICKETING-{self.instance_id}] Started\n'
                    f'   ğŸ“Š Event: {self.event_id}\n'
                    f'   ğŸ‘¥ Group: {self.consumer_group_id}\n'
                    f'   ğŸ”’ Processing: exactly-once\n'
                    f'   ğŸ“¦ Waiting for partition assignment...'
                )

                self.running = True
                if self.kafka_app:
                    Logger.base.info(
                        f'ğŸ¯ [TICKETING-{self.instance_id}] Running app\n'
                        f'   ğŸ’¡ Partition assignments will be logged when messages are processed'
                    )
                    self.kafka_app.run()
                    break  # Success, exit retry loop

            except Exception as e:
                error_msg = str(e)

                # Check if it's a topic metadata sync issue
                if 'UNKNOWN_TOPIC_OR_PART' in error_msg and attempt < max_retries:
                    Logger.base.warning(
                        f'âš ï¸ [TICKETING] Attempt {attempt}/{max_retries} failed: Topic metadata not ready\n'
                        f'   ğŸ”„ Retrying in {retry_delay}s... (Kafka brokers may still be syncing)'
                    )
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff

                    # Reset kafka_app for next attempt
                    self.kafka_app = None
                    continue
                else:
                    # Fatal error or max retries reached
                    Logger.base.error(f'âŒ [TICKETING] Start failed after {attempt} attempts: {e}')
                    raise

    def stop(self):
        """åœæ­¢æœå‹™"""
        if not self.running:
            return

        self.running = False

        try:
            Logger.base.info('ğŸ›‘ [TICKETING] Stopping consumer...')
            if self.kafka_app:
                self.kafka_app.stop()
            Logger.base.info('âœ… [TICKETING] Consumer stopped')
        except Exception as e:
            Logger.base.warning(f'âš ï¸ [TICKETING] Stop error: {e}')


# ============================================================
# Main Entry Point
# ============================================================


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    consumer = TicketingMqConsumer()

    try:
        # å•Ÿå‹• BlockingPortalï¼Œå‰µå»ºå…±äº«çš„ event loop
        with start_blocking_portal() as portal:
            consumer.set_portal(portal)
            consumer.start()

    except KeyboardInterrupt:
        Logger.base.info('âš ï¸ [TICKETING] Received interrupt signal')
        try:
            consumer.stop()
        except Exception:
            pass
    except Exception as e:
        Logger.base.error(f'ğŸ’¥ [TICKETING] Consumer error: {e}')
        try:
            consumer.stop()
        except:
            pass
    finally:
        Logger.base.info('ğŸ§¹ Cleaning up resources...')


if __name__ == '__main__':
    main()
