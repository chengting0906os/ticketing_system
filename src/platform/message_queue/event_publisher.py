"""
Domain Event Publisher
é ˜åŸŸäº‹ä»¶ç™¼å¸ƒå™¨

ç°¡åŒ–ç‰ˆäº‹ä»¶ç™¼å¸ƒæ¥å£ - ç›´æ¥ä½¿ç”¨ Quix Streams
"""

from datetime import datetime
from typing import Any, Literal

import attrs
from opentelemetry import trace
from opentelemetry.propagate import inject
from quixstreams import Application

from src.platform.config.core_setting import settings
from src.platform.logging.loguru_io import Logger
from src.service.shared_kernel.domain.domain_event import MqDomainEvent


# å…¨åŸŸ Quix Application å¯¦ä¾‹
_quix_app: Application | None = None


def _serialize_value(inst: type, field: attrs.Attribute, value: Any) -> Any:
    """
    è‡ªå®šç¾©åºåˆ—åŒ–å™¨ - å°‡ datetime è½‰æ›ç‚º timestamp

    ç”¨æ–¼ attrs.asdict çš„ value_serializer åƒæ•¸
    """
    if isinstance(value, datetime):
        return int(value.timestamp())
    return value


def _get_quix_app() -> Application:
    """å–å¾—å…¨åŸŸ Quix Application å¯¦ä¾‹ - æ”¯æ´ Exactly-Once èªç¾©"""
    global _quix_app
    if _quix_app is None:
        # ç”¢ç”Ÿå”¯ä¸€çš„äº‹å‹™ ID (ç”¨æ–¼ exactly-once)
        instance_id = settings.KAFKA_PRODUCER_INSTANCE_ID
        transactional_id = f'ticketing-producer-{instance_id}'

        _quix_app = Application(
            broker_address=settings.KAFKA_BOOTSTRAP_SERVERS,
            processing_guarantee='exactly-once',  # ğŸ†• å•Ÿç”¨ exactly-once
            producer_extra_config={
                'enable.idempotence': True,
                'acks': 'all',
                'retries': 3,
                'compression.type': 'snappy',
                'transactional.id': transactional_id,  # ğŸ†• äº‹å‹™ ID
                'max.in.flight.requests.per.connection': 5,  # ğŸ†• exactly-once å„ªåŒ–
            },
        )
    return _quix_app


@Logger.io
async def publish_domain_event(
    event: MqDomainEvent,
    topic: str,
    partition_key: str,
) -> Literal[True]:
    """
    ç™¼å¸ƒé ˜åŸŸäº‹ä»¶åˆ° Kafka

    Args:
        event: é ˜åŸŸäº‹ä»¶å°è±¡
        topic: Kafka topic åç¨±
        partition_key: åˆ†å€éµï¼ˆç¢ºä¿ç›¸åŒå¯¦é«”çš„äº‹ä»¶é †åºè™•ç†ï¼‰

    Returns:
        Trueï¼ˆæ°¸é æˆåŠŸï¼Œå¤±æ•—æœƒæ‹‹å‡ºç•°å¸¸ï¼‰

    ç¯„ä¾‹:
        await publish_domain_event(
            event=BookingCreated(booking_id=123, ...),
            topic=KafkaTopicBuilder.ticket_reserving_request(...),
            partition_key="booking-123"
        )
    """
    app = _get_quix_app()

    # å‰µå»º topicï¼ˆQuix æœƒè‡ªå‹•è™•ç†é‡è¤‡å‰µå»ºï¼‰
    kafka_topic = app.topic(
        name=topic,
        key_serializer='str',
        value_serializer='json',
    )

    # æº–å‚™äº‹ä»¶æ•¸æ“š - ä½¿ç”¨ attrs.asdict ä¸¦ç¢ºä¿ datetime è½‰æ›ç‚º timestamp
    event_dict = attrs.asdict(event, value_serializer=_serialize_value)

    event_data = {
        'event_type': event.__class__.__name__,
        'aggregate_id': str(event.aggregate_id),
        'occurred_at': int(event.occurred_at.timestamp()),
        **event_dict,
    }

    # ç§»é™¤å·²åŒ…å«çš„æ¬„ä½
    event_data.pop('aggregate_id', None)
    event_data.pop('occurred_at', None)

    # Inject trace context into Kafka headers for distributed tracing
    headers = {}
    inject(headers)  # Injects traceparent, tracestate into headers dict

    # Convert headers to Kafka format (list of tuples)
    kafka_headers = [
        (k, v.encode('utf-8') if isinstance(v, str) else v) for k, v in headers.items()
    ]

    # ç™¼å¸ƒäº‹ä»¶
    with app.get_producer() as producer:
        message = kafka_topic.serialize(
            key=partition_key,
            value=event_data,
        )

        producer.produce(
            topic=kafka_topic.name,
            key=message.key,
            value=message.value,
            headers=kafka_headers,  # Add trace context headers
        )

        producer.flush(timeout=5.0)

    current_span = trace.get_current_span()
    trace_id = format(current_span.get_span_context().trace_id, '032x') if current_span else 'none'
    Logger.base.info(
        f'ğŸ“¤ Published {event.__class__.__name__} to {topic} '
        f'(key: {partition_key}, trace_id: {trace_id})'
    )

    return True
