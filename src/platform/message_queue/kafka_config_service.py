"""
Kafka Configuration Service
ç‚ºæ´»å‹•è‡ªå‹•é…ç½® Kafka topics å’Œ consumers çš„æœå‹™
"""

import asyncio
from dataclasses import dataclass
import os
from typing import Dict, List

from src.platform.logging.loguru_io import Logger
from src.service.ticketing.shared_kernel.app.interface.i_kafka_config_service import (
    IKafkaConfigService,
)

from .kafka_constant_builder import KafkaTopicBuilder
from .section_based_partition_strategy import SectionBasedPartitionStrategy


@dataclass
class ConsumerConfig:
    name: str
    module: str
    description: str
    instance_count: int = 1  # é è¨­å•Ÿå‹• 1 å€‹å¯¦ä¾‹


class KafkaConfigService(IKafkaConfigService):
    """
    Kafka é…ç½®æœå‹™

    è² è²¬ç‚ºæ–°å‰µå»ºçš„æ´»å‹•è‡ªå‹•é…ç½®ï¼š
    1. Event-specific topics
    2. Section-based partition strategy
    3. Event-specific consumers
    """

    def __init__(self, total_partitions: int = 10):
        self.total_partitions = total_partitions
        self.partition_strategy = SectionBasedPartitionStrategy(total_partitions)

        # Consumer é…ç½®å®šç¾© - 1-2-1 æ¶æ§‹
        # booking: 1 consumer (è¼•é‡ç´šè¨‚å–®è™•ç†)
        # seat_reservation: 2 consumers (é«˜è² è¼‰åº§ä½é¸æ“‡ + Kvrocks æ“ä½œ)
        # event_ticketing: 1 consumer (ç‹€æ…‹ç®¡ç†)
        self.consumer_configs = [
            ConsumerConfig(
                name='ticketing_mq_consumer',
                module='src.service.ticketing.driving_adapter.mq_consumer.ticketing_mq_consumer',
                description='ğŸ« ç¥¨å‹™æœå‹™æ¶ˆè²»è€… (Booking + Ticket)',
                instance_count=1,
            ),
            ConsumerConfig(
                name='seat_reservation_mq_consumer',
                module='src.service.seat_reservation.driving_adapter.seat_reservation_mq_consumer',
                description='ğŸª‘ åº§ä½é è¨‚æ¶ˆè²»è€…',
                instance_count=1,
            ),
        ]

    @Logger.io
    async def setup_event_infrastructure(self, *, event_id: int, seating_config: Dict) -> bool:
        """
        ç‚ºæ–°æ´»å‹•è¨­ç½®å®Œæ•´çš„ Kafka åŸºç¤è¨­æ–½

        Returns:
            bool: æ˜¯å¦è¨­ç½®æˆåŠŸ
        """
        try:
            Logger.base.info(f'ğŸš€ [KAFKA_CONFIG] Setting up infrastructure for EVENT_ID={event_id}')

            # 1. å‰µå»º event-specific topics
            await self._create_event_topics(event_id)

            # 2. åˆ†æä¸¦å ±å‘Š partition ç­–ç•¥
            self._analyze_partition_distribution(event_id, seating_config)

            # 3. å•Ÿå‹• event-specific consumers
            await self._start_event_consumers(event_id)

            Logger.base.info(
                f'âœ… [KAFKA_CONFIG] Infrastructure setup completed for EVENT_ID={event_id}'
            )
            return True

        except Exception as e:
            Logger.base.error(
                f'âŒ [KAFKA_CONFIG] Failed to setup infrastructure for EVENT_ID={event_id}: {e}'
            )
            return False

    async def _create_event_topics(self, event_id: int) -> None:
        """å‰µå»º event-specific topics"""
        Logger.base.info(
            f'ğŸ¯ [KAFKA_CONFIG] Creating event-specific topics for EVENT_ID={event_id}'
        )

        topics = KafkaTopicBuilder.get_all_topics(event_id=event_id)

        # ä¸¦è¡Œå‰µå»ºæ‰€æœ‰ topics ä»¥æé«˜æ•ˆç‡
        tasks = [self._create_single_topic(topic) for topic in topics]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # çµ±è¨ˆçµæœ
        success_count = sum(1 for result in results if result is True)
        Logger.base.info(
            f'ğŸ“Š [KAFKA_CONFIG] Created {success_count}/{len(topics)} topics successfully'
        )

    async def _create_single_topic(self, topic: str) -> bool:
        """å‰µå»ºå–®å€‹ topic"""
        try:
            cmd = [
                'docker',
                'exec',
                'kafka1',
                'kafka-topics',
                '--bootstrap-server',
                'kafka1:29092',
                '--create',
                '--if-not-exists',
                '--topic',
                topic,
                '--partitions',
                str(self.total_partitions),
                '--replication-factor',
                '3',
            ]

            # ä½¿ç”¨ asyncio åŸ·è¡Œ subprocess
            process = await asyncio.create_subprocess_exec(
                *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
            )

            _, stderr = await asyncio.wait_for(process.communicate(), timeout=30)

            if process.returncode == 0:
                Logger.base.info(f'âœ… [KAFKA_CONFIG] Created topic: {topic}')
                return True
            else:
                Logger.base.warning(
                    f'âš ï¸ [KAFKA_CONFIG] Failed to create topic {topic}: {stderr.decode()}'
                )
                return False

        except asyncio.TimeoutError:
            Logger.base.error(f'âŒ [KAFKA_CONFIG] Timeout creating topic: {topic}')
            return False
        except Exception as e:
            Logger.base.error(f'âŒ [KAFKA_CONFIG] Error creating topic {topic}: {e}')
            return False

    def _analyze_partition_distribution(self, event_id: int, seating_config: Dict) -> None:
        """åˆ†æä¸¦è¨˜éŒ„ partition åˆ†ä½ˆç­–ç•¥"""
        Logger.base.info(
            f'ğŸ“Š [KAFKA_CONFIG] Analyzing partition distribution for EVENT_ID={event_id}'
        )

        # ç²å–å€åŸŸåˆ° partition çš„æ˜ å°„
        sections = seating_config.get('sections', [])
        mapping = self.partition_strategy.get_section_partition_mapping(sections, event_id)

        # è¨ˆç®—è² è¼‰åˆ†ä½ˆ
        loads = self.partition_strategy.calculate_expected_load(seating_config, event_id)

        # è¨˜éŒ„æ˜ å°„é—œä¿‚
        Logger.base.info('ğŸ—ºï¸ [KAFKA_CONFIG] Section-to-Partition Mapping:')
        for section, partition in mapping.items():
            Logger.base.info(f'   {section} å€ â†’ Partition {partition}')

        # è¨˜éŒ„è² è¼‰åˆ†ä½ˆ
        Logger.base.info('âš–ï¸ [KAFKA_CONFIG] Partition Load Distribution:')
        total_seats = 0
        for partition_id in sorted(loads.keys()):
            load_info = loads[partition_id]
            sections_str = ', '.join(load_info['sections'])
            seat_count = load_info['estimated_seats']
            total_seats += seat_count

            Logger.base.info(f'   Partition {partition_id}: {seat_count:,} seats ({sections_str})')

        Logger.base.info(f'ğŸ“ˆ [KAFKA_CONFIG] Total seats: {total_seats:,}')

    async def _start_event_consumers(self, event_id: int) -> None:
        """å•Ÿå‹• event-specific consumers (æ”¯æ´å¤šå¯¦ä¾‹)"""
        Logger.base.info(
            f'ğŸš€ [KAFKA_CONFIG] Starting event-specific consumers for EVENT_ID={event_id}'
        )

        # ç²å–é …ç›®æ ¹ç›®éŒ„
        project_root = self._get_project_root()

        # ç‚ºæ¯å€‹ consumer é…ç½®å‰µå»ºå¤šå€‹å¯¦ä¾‹çš„å•Ÿå‹•ä»»å‹™
        tasks = []
        for consumer in self.consumer_configs:
            for instance_id in range(1, consumer.instance_count + 1):
                tasks.append(
                    self._start_single_consumer(consumer, event_id, project_root, instance_id)
                )

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # çµ±è¨ˆçµæœ
        success_count = sum(
            1 for result in results if isinstance(result, int)
        )  # PID is returned on success
        total_instances = sum(c.instance_count for c in self.consumer_configs)
        Logger.base.info(
            f'ğŸ“Š [KAFKA_CONFIG] Started {success_count}/{total_instances} consumer instances successfully'
        )
        Logger.base.info(
            f'ğŸ”„ [KAFKA_CONFIG] Configuration: '
            f'ticketing:{self.consumer_configs[0].instance_count}, '
            f'seat_reservation:{self.consumer_configs[1].instance_count}'
        )

    async def _start_single_consumer(
        self, consumer: ConsumerConfig, event_id: int, project_root: str, instance_id: int
    ) -> int:
        """å•Ÿå‹•å–®å€‹ consumer å¯¦ä¾‹ï¼Œè¿”å› PID"""
        try:
            from .kafka_constant_builder import KafkaConsumerGroupBuilder

            cmd = ['uv', 'run', 'python', '-m', consumer.module]

            # è¨­ç½®ç’°å¢ƒè®Šæ•¸
            env = os.environ.copy()
            env['EVENT_ID'] = str(event_id)
            env['PYTHONPATH'] = project_root
            env['CONSUMER_INSTANCE_ID'] = str(instance_id)

            # æ ¹æ“šä¸åŒçš„ consumer è¨­ç½®ä¸åŒçš„ consumer group
            if 'ticketing' in consumer.name:
                env['CONSUMER_GROUP_ID'] = KafkaConsumerGroupBuilder.ticketing_service(
                    event_id=event_id
                )
            elif 'seat_reservation' in consumer.name:
                env['CONSUMER_GROUP_ID'] = KafkaConsumerGroupBuilder.seat_reservation_service(
                    event_id=event_id
                )

            # åœ¨èƒŒæ™¯å•Ÿå‹•ç¨‹åº
            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=project_root,
                env=env,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                start_new_session=True,  # è®“ç¨‹åºåœ¨èƒŒæ™¯ç¨ç«‹é‹è¡Œ
            )

            Logger.base.info(
                f'âœ… [KAFKA_CONFIG] Started {consumer.description} '
                f'instance {instance_id} for EVENT_ID={event_id}, PID={process.pid}'
            )

            return process.pid

        except Exception as e:
            Logger.base.error(
                f'âŒ [KAFKA_CONFIG] Failed to start {consumer.description} for EVENT_ID={event_id}: {e}'
            )
            raise

    def _get_project_root(self) -> str:
        """ç²å–é …ç›®æ ¹ç›®éŒ„"""
        # å¾ç•¶å‰æ–‡ä»¶ä½ç½®æ¨ç®—é …ç›®æ ¹ç›®éŒ„
        current_file = os.path.abspath(__file__)
        # å¾ src/platform/event_bus/kafka_config_service.py å›åˆ°æ ¹ç›®éŒ„
        return os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file))))

    def get_partition_key_for_seat(
        self, section: str, subsection: int, row: int, seat: int, event_id: int
    ) -> str:
        """
        ç‚ºåº§ä½ç”Ÿæˆ partition key
        ä½¿ç”¨å€åŸŸé›†ä¸­å¼ç­–ç•¥
        """
        return self.partition_strategy.generate_partition_key(
            section=section, subsection=subsection, row=row, seat=seat, event_id=event_id
        )

    async def get_active_consumer_groups(self) -> List[str]:
        """ç²å–æ‰€æœ‰æ´»èºçš„ consumer groups"""
        try:
            import subprocess

            cmd = [
                'docker',
                'exec',
                'kafka1',
                'kafka-consumer-groups',
                '--bootstrap-server',
                'kafka1:29092',
                '--list',
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                groups = [group.strip() for group in result.stdout.split('\n') if group.strip()]
                Logger.base.info(f'ğŸ“Š [KAFKA_CONFIG] Found {len(groups)} consumer groups: {groups}')
                return groups
            else:
                Logger.base.warning(
                    f'âš ï¸ [KAFKA_CONFIG] Failed to list consumer groups: {result.stderr}'
                )
                return []

        except Exception as e:
            Logger.base.error(f'âŒ [KAFKA_CONFIG] Error checking consumer groups: {e}')
            return []

    async def cleanup_event_infrastructure(self, event_id: int) -> bool:
        """
        æ¸…ç†æ´»å‹•çš„åŸºç¤è¨­æ–½
        (å¯é¸åŠŸèƒ½ï¼Œç”¨æ–¼æ´»å‹•çµæŸå¾Œçš„æ¸…ç†)
        """
        try:
            Logger.base.info(
                f'ğŸ§¹ [KAFKA_CONFIG] Cleaning up infrastructure for EVENT_ID={event_id}'
            )

            # 1. åœæ­¢ç›¸é—œçš„ consumers (é€šé PID æˆ–é€²ç¨‹åç¨±)
            # TODO: å¯¦ç¾ consumer åœæ­¢é‚è¼¯

            # 2. åˆªé™¤ event-specific topics (å¯é¸ï¼Œå–æ±ºæ–¼æ•¸æ“šä¿ç•™ç­–ç•¥)
            # TODO: å¯¦ç¾ topic åˆªé™¤é‚è¼¯

            Logger.base.info(
                f'âœ… [KAFKA_CONFIG] Infrastructure cleanup completed for EVENT_ID={event_id}'
            )
            return True

        except Exception as e:
            Logger.base.error(
                f'âŒ [KAFKA_CONFIG] Failed to cleanup infrastructure for EVENT_ID={event_id}: {e}'
            )
            return False
